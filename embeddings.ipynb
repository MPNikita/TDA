{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, BertModel\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка  nltk данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nmens\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nmens\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nmens\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработчик кодировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "FILES = [\n",
    "    [\"Biblioteka_prikluchenij.txt\", \"utf-8\"]\n",
    "    [\"detective_for_kidds.txt\", \"utf-8\"]\n",
    "    [\"detective_masters.txt\", \"windows-1251\"]\n",
    "    [\"russian_love_story.txt\", \"utf-8\"]\n",
    "]\n",
    "\n",
    "detector = UniversalDetector()\n",
    "\n",
    "for file in FILES:\n",
    "    with open(os.path.join(DATA_DIR, file), 'rb') as f:\n",
    "        for line in f:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "print(detector.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработчик текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_english(s: str) -> bool:\n",
    "    return any(c.isalpha() and c.isascii() for c in s)\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    text: str,\n",
    "    language: str = 'russian',\n",
    "    lemmatize: bool = True,\n",
    "    remove_stopwords: bool = True,\n",
    "    min_word_length: int = 2,\n",
    "    extra_stopwords: list = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Предобрабатывает текст для NLP-задач.\n",
    "    \n",
    "    Параметры:\n",
    "        text (str): Исходный текст\n",
    "        language (str): Язык текста ('russian' или 'english')\n",
    "        lemmatize (bool): Применять лемматизацию\n",
    "        remove_stopwords (bool): Удалять стоп-слова\n",
    "        min_word_length (int): Минимальная длина сохраняемых слов\n",
    "        extra_stopwords (list): Дополнительные стоп-слова\n",
    "    \n",
    "    Возвращает:\n",
    "        list: Список обработанных токенов\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Инициализация инструментов\n",
    "        morph = MorphAnalyzer() if lemmatize and language == 'russian' else None\n",
    "        stop_words = set(stopwords.words(language)) if remove_stopwords else set()\n",
    "        \n",
    "        # Добавление дополнительных стоп-слов\n",
    "        if extra_stopwords:\n",
    "            stop_words.update(extra_stopwords)\n",
    "\n",
    "        # Очистка текста\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zа-яё\\s]', ' ', text, flags=re.IGNORECASE)  # Удаление пунктуации и цифр\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Удаление лишних пробелов\n",
    "\n",
    "        # Токенизация\n",
    "        tokens = word_tokenize(text, language=language)\n",
    "\n",
    "        # Обработка токенов\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # Удаление коротких слов\n",
    "            if len(token) < min_word_length:\n",
    "                continue\n",
    "                \n",
    "            # Лемматизация\n",
    "            if lemmatize and morph:\n",
    "                lemma = morph.parse(token)[0].normal_form\n",
    "                token = lemma\n",
    "\n",
    "            # Удаление стоп-слов\n",
    "            if remove_stopwords and token in stop_words:\n",
    "                continue\n",
    "            \n",
    "            if has_english(token):\n",
    "                continue\n",
    "                \n",
    "            processed_tokens.append(token)\n",
    "\n",
    "        return processed_tokens\n",
    "\n",
    "    except LookupError as e:\n",
    "        print(f\"Ошибка: {e}. Проверьте установку необходимых ресурсов nltk.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Неизвестная ошибка: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    [\"Biblioteka_prikluchenij.txt\", \"utf-8\"],\n",
    "    [\"detective_for_kidds.txt\", \"utf-8\"],\n",
    "    [\"detective_masters.txt\", \"windows-1251\"],\n",
    "    [\"russian_love_story.txt\", \"utf-8\"]\n",
    "]\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "texts = []\n",
    "\n",
    "for filename, codec in FILES:\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        with open(path, 'r', encoding=codec) as f:\n",
    "            texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [preprocess_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocessed_data.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([' '.join(i) for i in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df=3,           \n",
    "    max_features=5000   \n",
    ")\n",
    "\n",
    "processed_texts = [' '.join(sentence) for sentence in sentences]\n",
    "tfidf_matrix = tfidf.fit_transform(processed_texts)\n",
    "\n",
    "tfidf_vec = {}\n",
    "for word in tfidf.get_feature_names_out():\n",
    "    idx = tfidf.vocabulary_[word]\n",
    "    vector = tfidf_matrix[:, idx].toarray().flatten()\n",
    "    tfidf_vec[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save(os.path.join('models', 'word2vec.model'))\n",
    "fasttext_model.save(os.path.join('models', 'fasttext.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vec.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение обработанных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocessed_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформеры (ruBERT и fine tuned). Загрузка берта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "stock_model_name = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "os.makedirs('./stock_bert', exist_ok=True)\n",
    "os.makedirs('./finetuned_bert', exist_ok=True)\n",
    "\n",
    "stock_tokenizer = BertTokenizer.from_pretrained(stock_model_name)\n",
    "stock_model = BertForMaskedLM.from_pretrained(stock_model_name)\n",
    "\n",
    "stock_tokenizer.save_pretrained('./stock_bert')\n",
    "stock_model.save_pretrained('./stock_bert', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файн тюнинг. Подготовка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac95d6162bd4933b5da2a098ba692f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": [\" \".join(text) for text in sentences]})\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение токенизированных данных для Берт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tokenized_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файн тюнинг. Обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=4.736667633056641, metrics={'train_runtime': 49.1246, 'train_samples_per_second': 0.814, 'train_steps_per_second': 0.407, 'total_flos': 1317391488000.0, 'train_loss': 4.736667633056641, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "with open(\"data/tokenized_dataset.pkl\", \"rb\") as f:\n",
    "    tokens = pickle.load(f)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokens,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение Берта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_bert\\\\tokenizer_config.json',\n",
       " './finetuned_bert\\\\special_tokens_map.json',\n",
       " './finetuned_bert\\\\vocab.txt',\n",
       " './finetuned_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./finetuned_bert', safe_serialization=True)\n",
    "tokenizer.save_pretrained('./finetuned_bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание вложений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка и загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20419it [1:21:55,  4.15it/s] \n"
     ]
    }
   ],
   "source": [
    "dm_path = \"data/adjectives100_ruscorp_add_ppmi_300.dm\"\n",
    "phrase_vectors = {}\n",
    "data = {}\n",
    "vectors = {'initial' : list(),\n",
    "            'wor2vec' : list(),\n",
    "            'stock_bert' : list(),\n",
    "            'finetuned_bert' : list(),\n",
    "            'tf_idf' : list(),\n",
    "            'fasttext': list()}\n",
    "\n",
    "with open(dm_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split()\n",
    "        phrase = parts[0]\n",
    "        adjective, noun = phrase.split('_', 1)\n",
    "        word = preprocess_text(noun)\n",
    "        if not word:\n",
    "            continue\n",
    "        word = word[0]\n",
    "        \n",
    "        vector = list(map(float, parts[1:]))\n",
    "        \n",
    "        if not data.get(adjective, None):\n",
    "            data[adjective] = dict()\n",
    "        \n",
    "        data[adjective][word] = vectors\n",
    "        data[adjective][word]['initial'] = np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание вложений по моделям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 883.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово коф не найдено\n",
      "Слово беспризорность не найдено\n",
      "Слово млрд не найдено\n",
      "Слово кейвинга не найдено\n",
      "Слово спелеотуризм не найдено\n",
      "Слово ество не найдено\n",
      "Слово землеведение не найдено\n",
      "Слово боев не найдено\n",
      "Слово чуйка не найдено\n",
      "Слово юечж не найдено\n",
      "Слово токмак не найдено\n",
      "Слово лиличка не найдено\n",
      "Слово окный не найдено\n",
      "Слово высокопочитание не найдено\n",
      "Слово хабуг не найдено\n",
      "Слово жиган не найдено\n",
      "Слово рамень не найдено\n",
      "Слово коф не найдено\n",
      "Слово отесинька не найдено\n",
      "Слово упк не найдено\n",
      "Слово маргарит не найдено\n",
      "Слово фазис не найдено\n",
      "Слово уренгой не найдено\n",
      "Слово парадигма не найдено\n",
      "Слово коап не найдено\n",
      "Слово леф не найдено\n",
      "Слово репатриант не найдено\n",
      "Слово миров не найдено\n",
      "Слово стат не найдено\n",
      "Слово лукий не найдено\n",
      "Слово бох не найдено\n",
      "Слово монархиня не найдено\n",
      "Слово честие не найдено\n",
      "Слово новагород не найдено\n",
      "Слово радостие не найдено\n",
      "Слово старчество не найдено\n",
      "Слово песнотворец не найдено\n",
      "Слово алфим не найдено\n",
      "Слово вертереть не найдено\n",
      "Слово моща не найдено\n",
      "Слово экзерциция не найдено\n",
      "Слово окный не найдено\n",
      "Слово окный не найдено\n",
      "Слово окный не найдено\n",
      "Слово галстуха не найдено\n",
      "Слово эса не найдено\n",
      "Слово эсэр не найдено\n",
      "Слово герпес не найдено\n",
      "Слово губаха не найдено\n",
      "Слово коф не найдено\n",
      "Слово окный не найдено\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "for adj in tqdm(data.keys()):\n",
    "    for word in data[adj].keys():\n",
    "        try:\n",
    "            vector = model.wv[word]\n",
    "            vector = vector / np.linalg.norm(vector)\n",
    "            data[adj][word]['wor2vec'] = vector\n",
    "        except:\n",
    "            print(f\"Слово {word} не найдено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:00<00:00, 749.83it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText.load(\"models/fasttext.model\")\n",
    "\n",
    "for adj in tqdm(data.keys()):\n",
    "    for word in data[adj].keys():\n",
    "        vector = fasttext_model.wv[word]\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        data[adj][word]['fasttext'] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adj in tqdm(data.keys()):\n",
    "    for word in data[adj].keys():\n",
    "        try:\n",
    "            vector = tfidf_vec[word]\n",
    "            vector = vector / np.linalg.norm(vector)\n",
    "            data[adj][word]['tf_idf'] = vector\n",
    "        except:\n",
    "            print(f\"Слово {word} не найдено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ruBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(word, tokenizer, model):\n",
    "    inputs = tokenizer(\n",
    "        word, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at stock_bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "stock_tokenizer = BertTokenizer.from_pretrained('stock_bert')\n",
    "stock_model = BertModel.from_pretrained('stock_bert')\n",
    "\n",
    "finetuned_tokenizer = BertTokenizer.from_pretrained('finetuned_bert')\n",
    "finetuned_model = BertModel.from_pretrained('finetuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [21:02<00:00, 12.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for adj in tqdm(data.keys()):\n",
    "    for noun in data[adj].keys():\n",
    "        try:\n",
    "            stock_vec = get_bert_embedding(noun, stock_tokenizer, stock_model)\n",
    "            stock_vec = stock_vec / np.linalg.norm(stock_vec)\n",
    "            data[adj][noun]['stock_bert'] = stock_vec\n",
    "            \n",
    "            finetuned_vec = get_bert_embedding(noun, finetuned_tokenizer, finetuned_model)\n",
    "            finetuned_vec = finetuned_vec / np.linalg.norm(finetuned_vec)\n",
    "            data[adj][noun]['finetuned_bert'] = finetuned_vec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка для {noun}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение полученного словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/embeddings_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
