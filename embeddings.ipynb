{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import DataCollatorForLanguageModeling, BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, BertModel\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∫–∞  nltk –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nmens\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nmens\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nmens\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–¥–∏—Ä–æ–≤–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "FILES = [\n",
    "    [\"Biblioteka_prikluchenij.txt\", \"utf-8\"]\n",
    "    [\"detective_for_kidds.txt\", \"utf-8\"]\n",
    "    [\"detective_masters.txt\", \"windows-1251\"]\n",
    "    [\"russian_love_story.txt\", \"utf-8\"]\n",
    "]\n",
    "\n",
    "detector = UniversalDetector()\n",
    "\n",
    "for file in FILES:\n",
    "    with open(os.path.join(DATA_DIR, file), 'rb') as f:\n",
    "        for line in f:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "print(detector.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_english(s: str) -> bool:\n",
    "    return any(c.isalpha() and c.isascii() for c in s)\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    text: str,\n",
    "    language: str = 'russian',\n",
    "    lemmatize: bool = True,\n",
    "    remove_stopwords: bool = True,\n",
    "    min_word_length: int = 2,\n",
    "    extra_stopwords: list = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è NLP-–∑–∞–¥–∞—á.\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "        text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "        language (str): –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ ('russian' –∏–ª–∏ 'english')\n",
    "        lemmatize (bool): –ü—Ä–∏–º–µ–Ω—è—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é\n",
    "        remove_stopwords (bool): –£–¥–∞–ª—è—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "        min_word_length (int): –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö —Å–ª–æ–≤\n",
    "        extra_stopwords (list): –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        list: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n",
    "        morph = MorphAnalyzer() if lemmatize and language == 'russian' else None\n",
    "        stop_words = set(stopwords.words(language)) if remove_stopwords else set()\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "        if extra_stopwords:\n",
    "            stop_words.update(extra_stopwords)\n",
    "\n",
    "        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z–∞-—è—ë\\s]', ' ', text, flags=re.IGNORECASE)  # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ü–∏—Ñ—Ä\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\n",
    "\n",
    "        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "        tokens = word_tokenize(text, language=language)\n",
    "\n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤\n",
    "            if len(token) < min_word_length:\n",
    "                continue\n",
    "                \n",
    "            # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è\n",
    "            if lemmatize and morph:\n",
    "                lemma = morph.parse(token)[0].normal_form\n",
    "                token = lemma\n",
    "\n",
    "            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤\n",
    "            if remove_stopwords and token in stop_words:\n",
    "                continue\n",
    "            \n",
    "            if has_english(token):\n",
    "                continue\n",
    "                \n",
    "            processed_tokens.append(token)\n",
    "\n",
    "        return processed_tokens\n",
    "\n",
    "    except LookupError as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞: {e}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ nltk.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    [\"Biblioteka_prikluchenij.txt\", \"utf-8\"],\n",
    "    [\"detective_for_kidds.txt\", \"utf-8\"],\n",
    "    [\"detective_masters.txt\", \"windows-1251\"],\n",
    "    [\"russian_love_story.txt\", \"utf-8\"]\n",
    "]\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "texts = []\n",
    "\n",
    "for filename, codec in FILES:\n",
    "        path = os.path.join(DATA_DIR, filename)\n",
    "        with open(path, 'r', encoding=codec) as f:\n",
    "            texts.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [preprocess_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    sentences=sentences,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocessed_data.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([' '.join(i) for i in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    min_df=3,           \n",
    "    max_features=5000   \n",
    ")\n",
    "\n",
    "processed_texts = [' '.join(sentence) for sentence in sentences]\n",
    "tfidf_matrix = tfidf.fit_transform(processed_texts)\n",
    "\n",
    "tfidf_vec = {}\n",
    "for word in tfidf.get_feature_names_out():\n",
    "    idx = tfidf.vocabulary_[word]\n",
    "    vector = tfidf_matrix[:, idx].toarray().flatten()\n",
    "    tfidf_vec[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.save(os.path.join('models', 'word2vec.model'))\n",
    "fasttext_model.save(os.path.join('models', 'fasttext.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vec.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/preprocessed_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã (ruBERT –∏ fine tuned). –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ—Ä—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "stock_model_name = 'DeepPavlov/rubert-base-cased'\n",
    "\n",
    "os.makedirs('./stock_bert', exist_ok=True)\n",
    "os.makedirs('./finetuned_bert', exist_ok=True)\n",
    "\n",
    "stock_tokenizer = BertTokenizer.from_pretrained(stock_model_name)\n",
    "stock_model = BertForMaskedLM.from_pretrained(stock_model_name)\n",
    "\n",
    "stock_tokenizer.save_pretrained('./stock_bert')\n",
    "stock_model.save_pretrained('./stock_bert', safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–∞–π–Ω —Ç—é–Ω–∏–Ω–≥. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac95d6162bd4933b5da2a098ba692f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": [\" \".join(text) for text in sentences]})\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ë–µ—Ä—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/tokenized_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenized_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–§–∞–π–Ω —Ç—é–Ω–∏–Ω–≥. –û–±—É—á–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=4.736667633056641, metrics={'train_runtime': 49.1246, 'train_samples_per_second': 0.814, 'train_steps_per_second': 0.407, 'total_flos': 1317391488000.0, 'train_loss': 4.736667633056641, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "\n",
    "with open(\"data/tokenized_dataset.pkl\", \"rb\") as f:\n",
    "    tokens = pickle.load(f)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-finetuned',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokens,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ë–µ—Ä—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./finetuned_bert\\\\tokenizer_config.json',\n",
       " './finetuned_bert\\\\special_tokens_map.json',\n",
       " './finetuned_bert\\\\vocab.txt',\n",
       " './finetuned_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./finetuned_bert', safe_serialization=True)\n",
    "tokenizer.save_pretrained('./finetuned_bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–∏–π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20419it [1:21:55,  4.15it/s] \n"
     ]
    }
   ],
   "source": [
    "dm_path = \"data/adjectives100_ruscorp_add_ppmi_300.dm\"\n",
    "phrase_vectors = {}\n",
    "data = {}\n",
    "vectors = {'initial' : list(),\n",
    "            'wor2vec' : list(),\n",
    "            'stock_bert' : list(),\n",
    "            'finetuned_bert' : list(),\n",
    "            'tf_idf' : list(),\n",
    "            'fasttext': list()}\n",
    "\n",
    "with open(dm_path, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        parts = line.strip().split()\n",
    "        phrase = parts[0]\n",
    "        adjective, noun = phrase.split('_', 1)\n",
    "        word = preprocess_text(noun)\n",
    "        if not word:\n",
    "            continue\n",
    "        word = word[0]\n",
    "        \n",
    "        vector = list(map(float, parts[1:]))\n",
    "        \n",
    "        if not data.get(adjective, None):\n",
    "            data[adjective] = dict()\n",
    "        \n",
    "        data[adjective][word] = vectors\n",
    "        data[adjective][word]['initial'] = np.array(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–∏–π –ø–æ –º–æ–¥–µ–ª—è–º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00<00:00, 883.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–ª–æ–≤–æ –∫–æ—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –±–µ—Å–ø—Ä–∏–∑–æ—Ä–Ω–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –º–ª—Ä–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∫–µ–π–≤–∏–Ω–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Å–ø–µ–ª–µ–æ—Ç—É—Ä–∏–∑–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –µ—Å—Ç–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∑–µ–º–ª–µ–≤–µ–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –±–æ–µ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —á—É–π–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —é–µ—á–∂ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Ç–æ–∫–º–∞–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –ª–∏–ª–∏—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –æ–∫–Ω—ã–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –≤—ã—Å–æ–∫–æ–ø–æ—á–∏—Ç–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Ö–∞–±—É–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∂–∏–≥–∞–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Ä–∞–º–µ–Ω—å –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∫–æ—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –æ—Ç–µ—Å–∏–Ω—å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —É–ø–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –º–∞—Ä–≥–∞—Ä–∏—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Ñ–∞–∑–∏—Å –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —É—Ä–µ–Ω–≥–æ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –ø–∞—Ä–∞–¥–∏–≥–º–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∫–æ–∞–ø –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –ª–µ—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Ä–µ–ø–∞—Ç—Ä–∏–∞–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –º–∏—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Å—Ç–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –ª—É–∫–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –±–æ—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –º–æ–Ω–∞—Ä—Ö–∏–Ω—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —á–µ—Å—Ç–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –Ω–æ–≤–∞–≥–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Ä–∞–¥–æ—Å—Ç–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —Å—Ç–∞—Ä—á–µ—Å—Ç–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –ø–µ—Å–Ω–æ—Ç–≤–æ—Ä–µ—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∞–ª—Ñ–∏–º –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –≤–µ—Ä—Ç–µ—Ä–µ—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –º–æ—â–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —ç–∫–∑–µ—Ä—Ü–∏—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –æ–∫–Ω—ã–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –æ–∫–Ω—ã–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –æ–∫–Ω—ã–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –≥–∞–ª—Å—Ç—É—Ö–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —ç—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ —ç—Å—ç—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –≥–µ—Ä–ø–µ—Å –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –≥—É–±–∞—Ö–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –∫–æ—Ñ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n",
      "–°–ª–æ–≤–æ –æ–∫–Ω—ã–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"models/word2vec.model\")\n",
    "\n",
    "for adj in tqdm(data.keys()):\n",
    "    for word in data[adj].keys():\n",
    "        try:\n",
    "            vector = model.wv[word]\n",
    "            vector = vector / np.linalg.norm(vector)\n",
    "            data[adj][word]['wor2vec'] = vector\n",
    "        except:\n",
    "            print(f\"–°–ª–æ–≤–æ {word} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00<00:00, 749.83it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText.load(\"models/fasttext.model\")\n",
    "\n",
    "for adj in tqdm(data.keys()):\n",
    "    for word in data[adj].keys():\n",
    "        vector = fasttext_model.wv[word]\n",
    "        vector = vector / np.linalg.norm(vector)\n",
    "        data[adj][word]['fasttext'] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adj in tqdm(data.keys()):\n",
    "    for word in data[adj].keys():\n",
    "        try:\n",
    "            vector = tfidf_vec[word]\n",
    "            vector = vector / np.linalg.norm(vector)\n",
    "            data[adj][word]['tf_idf'] = vector\n",
    "        except:\n",
    "            print(f\"–°–ª–æ–≤–æ {word} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ruBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(word, tokenizer, model):\n",
    "    inputs = tokenizer(\n",
    "        word, \n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    return embeddings.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at stock_bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "stock_tokenizer = BertTokenizer.from_pretrained('stock_bert')\n",
    "stock_model = BertModel.from_pretrained('stock_bert')\n",
    "\n",
    "finetuned_tokenizer = BertTokenizer.from_pretrained('finetuned_bert')\n",
    "finetuned_model = BertModel.from_pretrained('finetuned_bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [21:02<00:00, 12.75s/it]\n"
     ]
    }
   ],
   "source": [
    "for adj in tqdm(data.keys()):\n",
    "    for noun in data[adj].keys():\n",
    "        try:\n",
    "            stock_vec = get_bert_embedding(noun, stock_tokenizer, stock_model)\n",
    "            stock_vec = stock_vec / np.linalg.norm(stock_vec)\n",
    "            data[adj][noun]['stock_bert'] = stock_vec\n",
    "            \n",
    "            finetuned_vec = get_bert_embedding(noun, finetuned_tokenizer, finetuned_model)\n",
    "            finetuned_vec = finetuned_vec / np.linalg.norm(finetuned_vec)\n",
    "            data[adj][noun]['finetuned_bert'] = finetuned_vec\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –¥–ª—è {noun}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/embeddings_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
